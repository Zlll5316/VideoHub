import os
import time
import requests
import uvicorn
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

# ==========================================
# ğŸš¨ é…ç½®åŒºåŸŸ (ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œä¸è¦ç¡¬ç¼–ç å¯†é’¥)
# ==========================================
# ä»ç¯å¢ƒå˜é‡è¯»å– Notion API Keyï¼ˆåœ¨æœ¬åœ°å¼€å‘æ—¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼‰
NOTION_API_KEY = os.getenv("NOTION_API_KEY", "")
if not NOTION_API_KEY:
    print("âš ï¸ è­¦å‘Š: NOTION_API_KEY æœªè®¾ç½®ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡")
DATABASE_ID = os.getenv("DATABASE_ID", "2d3e8a9a934180f08bf0f20a67aa1c62")

MY_PROXIES = {
    "http": "http://10.20.160.120:8118",
    "https": "http://10.20.160.120:8118"
}
os.environ["http_proxy"] = MY_PROXIES["http"]
os.environ["https_proxy"] = MY_PROXIES["https"]

CACHE_DURATION = 300  
# ==========================================

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

NOTION_HEADERS = {
    "Authorization": f"Bearer {NOTION_API_KEY}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

global_cache = {"data": [], "last_updated": 0}

def get_youtube_thumbnail(url):
    try:
        video_id = ""
        if "youtu.be" in url:
            video_id = url.split("/")[-1].split("?")[0]
        elif "v=" in url:
            video_id = url.split("v=")[1].split("&")[0]
        if video_id:
            return f"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg"
    except:
        pass
    return ""

# ğŸ› ï¸ è¾…åŠ©å‡½æ•°ï¼šé€šç”¨æ ‡ç­¾è§£æ (æ”¯æŒ Multi-select å’Œ Select)
def parse_multi_select(prop_data):
    if not prop_data: return []
    # å¦‚æœæ˜¯å¤šé€‰ (Multi-select)
    if prop_data.get("type") == "multi_select":
        return [t['name'] for t in prop_data.get("multi_select", [])]
    # å¦‚æœæ˜¯å•é€‰ (Select)
    elif prop_data.get("type") == "select":
        select_obj = prop_data.get("select")
        return [select_obj['name']] if select_obj else []
    return []

@app.get("/fetch_video_list")
async def fetch_notion_data():
    current_time = time.time()
    
    if current_time - global_cache["last_updated"] < CACHE_DURATION and global_cache["data"]:
        print(f"ğŸš€ [é«˜é€Ÿ] ä½¿ç”¨æœ¬åœ°ç¼“å­˜")
        return {"status": "success", "data": global_cache["data"]}

    print(f"\nğŸ”„ [åŠ è½½ä¸­] æ­£åœ¨è¿æ¥ Notion (ID: {DATABASE_ID})...")
    print(f"ğŸ”‘ ä½¿ç”¨ Token: {NOTION_API_KEY[:10]}...") 

    url = f"https://api.notion.com/v1/databases/{DATABASE_ID}/query"
    
    try:
        response = requests.post(url, headers=NOTION_HEADERS, proxies=MY_PROXIES, timeout=30)
        
        if response.status_code != 200:
            print(f"âŒ è¯»å–å¤±è´¥ (ä»£ç  {response.status_code}): {response.text}")
            return {"status": "error", "message": f"API token is invalid or network error."}

        data = response.json()
        results = data.get("results", [])
        print(f"âœ… æˆåŠŸè¯»å–åˆ° {len(results)} æ¡æ•°æ®ï¼Œæ­£åœ¨åˆ†ç±»è§£æ...")

        clean_videos = []
        for page in results:
            props = page.get("properties", {})
            
            # 1. åŸºç¡€ä¿¡æ¯
            title = "æ— æ ‡é¢˜"
            name_col = props.get("åç§°", {})
            if name_col.get("title"):
                title = name_col["title"][0].get("plain_text", "æ— æ ‡é¢˜")

            video_url = ""
            url_col = props.get("URL", {})
            if url_col.get("url"): video_url = url_col["url"]
            elif url_col.get("rich_text") and len(url_col["rich_text"]) > 0:
                video_url = url_col["rich_text"][0].get("plain_text", "")

            analysis = "æš‚æ— åˆ†æå†…å®¹"
            analysis_col = props.get("è§†é¢‘åˆ†æ", {})
            if analysis_col.get("rich_text"):
                analysis = "".join([t.get("plain_text", "") for t in analysis_col["rich_text"]])

            # 2. ğŸ‘‡ æ ¸å¿ƒä¿®æ”¹ï¼šè¯»å–4ä¸ªç‹¬ç«‹çš„åˆ†ç±»åˆ—
            # æ³¨æ„ï¼šè¿™é‡Œä¼šå°è¯•å…¼å®¹ å•é€‰(Select) å’Œ å¤šé€‰(Multi-select)
            company_tags = parse_multi_select(props.get("å…¬å¸", {}))
            type_tags = parse_multi_select(props.get("åŠ¨ç”»ç±»å‹", {}))
            technique_tags = parse_multi_select(props.get("è¡¨ç°æ‰‹æ³•", {}))
            feature_tags = parse_multi_select(props.get("å…¸å‹ç‰¹å¾", {}))

            # 3. å°é¢å¤„ç†
            cover_img = ""
            cover_data = page.get("cover", {})
            if cover_data:
                if cover_data['type'] == 'external': cover_img = cover_data['external']['url']
                elif cover_data['type'] == 'file': cover_img = cover_data['file']['url']
            
            if not cover_img and video_url:
                cover_img = get_youtube_thumbnail(video_url)
            if not cover_img:
                cover_img = "https://images.unsplash.com/photo-1618005182384-a83a8bd57fbe?q=80&w=800&auto=format&fit=crop"

            clean_videos.append({
                "id": page["id"],
                "title": title,
                "url": video_url,
                "analysis": analysis,
                "cover": cover_img,
                # ğŸ‘‡ å°†4ä¸ªåˆ†ç±»åˆ†åˆ«ä¼ ç»™å‰ç«¯
                "company": company_tags,
                "animationType": type_tags,
                "technique": technique_tags,
                "features": feature_tags
            })

        global_cache["data"] = clean_videos
        global_cache["last_updated"] = current_time
        
        return {"status": "success", "data": clean_videos}

    except Exception as e:
        print(f"âŒ ä»£ç æŠ¥é”™: {e}")
        return {"status": "error", "message": str(e)}

@app.get("/health")
def health_check(): return {"status": "ok"}

if __name__ == "__main__":
    os.system("lsof -ti:8000 | xargs kill -9") 
    uvicorn.run(app, host="0.0.0.0", port=8000)